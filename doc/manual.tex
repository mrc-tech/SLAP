\documentclass[10pt]{article}

\usepackage[a4paper,top=2.5cm,bottom=3.5cm,left=2.5cm,right=2.5cm]{geometry}
%\usepackage[T1]{fontenc}		%implementa nei font gli accenti (crea un casino facendo tutto pixellato)
\usepackage[utf8]{inputenc}		%importa i caratteri utf
\usepackage{inputenc}
\usepackage[italian]{babel}
\usepackage{amsmath,amssymb}	%per visualizzare la matematica (per text{})
\usepackage{mathtools}
%\usepackage{graphicx}			%per le figure
\usepackage{xcolor}				%per scrivere colorato XCOLOR per lstlisting colorato (se non c'è tikz)
\usepackage[toc,page]{appendix}	%per l'implementazione delle appendici
%\usepackage{subfig}				%per figure multiple in una stessa figure
%\usepackage{pdfpages}			%per includere pezzi di pdf
%\usepackage{float}				%per mettere le figure nel posto giusto
\usepackage{tikz}				%per le immagini vettoriali da LateX
\usepackage{listings} 			%Per inserire codice
\usepackage{pxfonts}			%per il bold nei lstlisting
%\usepackage{scrextend}			%per labeling
\usepackage{enumitem}			% per [noitemsep]
\usepackage{algorithm}			% per la descrizione degli algoritmi
\usepackage{algpseudocode}		% per lo pseudocodice


%\definecolor{mymauve}{rgb}{0.58,0,0.82}

\lstset { %
	language=C++,
	backgroundcolor=\color{black!5}, % set backgroundcolor
	basicstyle=\footnotesize\ttfamily, % basic font setting
	frame=shadowbox,
	breaklines= true, % va a capo automatico
	numbers=left, % numera le righe
	commentstyle=\color{gray}, % commenti grigi
	stringstyle=\color{mymauve}, % string literal style
	numberstyle=\footnotesize\ttfamily, %formato dei numeri a lato delle righe
	keywordstyle=\color{black}\bfseries, % style for keywords
}




\begin{document}


\title{\textbf{SLAP}\\ Simple Linear Algebra Package}
\author{Andrea Marchi}

\maketitle

\tableofcontents



\section{Data type}

Le matrici sono salvate in array monodimensionali \textit{row-major}, ovvero che i dati sulle righe sono sequenziali. Quindi nella indicizzazione degli elementi della matrice con $n$ righe e $m$ colonne ($\mathbb{R}^{n\times m}$) si usa la formula:
\begin{lstlisting}
matrix[i][j] = array[i*m + j]
\end{lstlisting}
Le righe vanno da $0$ a $n-1$, mentre le colonne vanno da $0$ a $m-1$.

Alcune librerie usano la notazione \textit{column-major}, ovvero con indicizzazione \verb|array[j*n+i]|. Avere una funzione che organizza i dati della matrice in colonne o in righe è comodo per quanto riguarda l'ottimizzazione della cache di lettura dei dati sequenziali dalla RAM alla CPU.

Il tipo di dati (essendo in C) è una struttura (\verb|struct|) e la definizione cambia al variare del tipo di dati base (il C non permette l'uso di template).
La struttura base è:
\begin{lstlisting}
typedef struct _matd{
	unsigned int n_rows;
	unsigned int n_cols;
	double *data; // row-major matrix data array
} matd;
\end{lstlisting}
dove la lettera finale indica il tipo di variabile usata, in questo caso \verb|double|. I tipi di dati che ha senso utilizzare nella libreria sono:
\begin{description}[noitemsep]
\item[\texttt{d}] virgola mobile a doppia precisione (\verb|double|)
\item[\texttt{f}] virgola mobile (\verb|float|)
\item[\texttt{i}] intero (\verb|int|)
\item[\texttt{b}] byte (\verb|unsigned char|)
\end{description}

Per allocare la memoria e liberarla (sempre liberare la memoria dopo averla allocata):
\begin{lstlisting}
matd* new_matd(unsigned int num_rows, unsigned int num_cols)
{
	int i;
	// create a new double matrix
	if(num_rows == 0) { /*SLAP_ERROR(INVALID_ROWS);*/ return 0; } // dovrebbe ritornare NULL
	if(num_cols == 0) { /*SLAP_ERROR(INVALID_COLS);*/ return 0; } // dovrebbe ritornare NULL
	
	matd *m = calloc(1, sizeof(*m)); // allocate space for the struct
	// CONTROLLARE LA MATRICE CREATA ( SLAP_CHECK(m) )
	m->n_rows = num_rows;
	m->n_cols = num_cols;
	m->data = calloc(m->n_rows*m->n_cols, sizeof(*m->data));
	// CONTROLLARE I DATI CREATI ( SLAP_CHECK(m->data) )
	for(i=0; i<num_rows*num_cols; i++) m->data[i] = 0; // set to zero
	
	return m;
}

void free_mat(matd *matrix)
{
	free(matrix->data); // delete the data
	free(matrix); // delete the data structure
}
\end{lstlisting}

Come setters e getters non potendo usare le operation del C++ e non riuscendo a fare qualcosa di funzionante e decente con le macro\footnote{Usare le macro mi permetterebbe di risparmiare tempo nella allocazione dei parametri delle funzioni. Negli algoritmi potrei usare l'accesso diretto all'array \texttt{data}.} uso le funzioni
\begin{lstlisting}
double matd_get(matd matrix, unsigned int row, unsigned int col) {
	return matrix.data[row*matrix.n_cols + col]; // row-major
}
void matd_set(matd matrix, unsigned int row, unsigned int col, double val) {
	matrix.data[row*matrix.n_cols + col] = val; // row-major
}
\end{lstlisting}
\textcolor{red}{Dovrei controllare che l'accesso sia corretto (che non cerchi di scrivere/leggere dati non allocati).}



\section{Basic operations}

\textcolor{red}{Mettere un pannello (anche una tabella) che riassume le operazioni, il nome delle funzioni e come si usano.}

\subsection{Equality}

\subsection{Addition and sottration}

\subsection{Multiplication}

\subsubsection{Scalar-matrix multiplication}
\subsubsection{Vector-matrix multiplication}

La moltiplicazione matrice-vettore può essere vista come la moltiplicazione tra una matrice e un'altra nella forma di un vettore colonna.
\textcolor{red}{Fare un controllo numerico su questa affermazione}

\subsubsection{Matrix-matrix multiplication}

Il prodotto tra due matrici $\mathbf{A} \in \mathbb{R}^{n\times m}$ (matrice con $n$ righe e $m$ colonne) $\mathbf{B} \in \mathbb{R}^{m\times p}$ ($m$ righe, $p$ colonne) è una matrice $\mathbf{C} \in \mathbb{R}^{n\times p}$ le cui componenti sono definite come:
\begin{equation}
C_{ij} = \sum_{k=1}^m A_{ik} \; B_{kj}
\end{equation}

Applicando direttamente la definizione della moltiplicazione tra matrici si ottiene un algoritmo che ha un'efficienza computazionale di $O(n^3)$ (dove $n$ è la dimensione di una matrice quadrata $n\times n$). Migliori limiti asintotici sono stati scoperti dopo l'algoritmo di Strassen negli anni '60 (il limite teorico rimane ancora ignoto). Recentemente è stato annunciato un algoritmo con l'efficienza teorica di $O(n^{2.37188})$, ma si tratta di un \emph{algoritmo galattico}, ovvero che richiede una dimensione talmente grande delle matrici per funzionare con tale efficienza da non essere praticamente realizzabile (in questo caso a causa di una grossa costante).

\paragraph{Simple algorithm:} Dalla definizione della moltiplicazione tra matrici si ricava direttamente l'algoritmo~\ref{alg:mul_simple}.

\begin{algorithm}
\caption{Semplice algoritmo per la moltiplicazione tra matrici}\label{alg:mul_simple}
\begin{algorithmic}[1]
\Require matrix $\mathbf{A}$, matrix $\mathbf{B}$
\State $\mathbf{C} \gets $ matrix of the appripriate size
\For{$i \gets 1$ to $n$} \Comment{nell'implementazione parte da $0$ e arriva a $n-1$}
	\For{$j \gets 1$ to $p$} \Comment{stessa considerazione dell'indice $i$}
		\State $sum \gets 0$
		\For{$k \gets 1$ to $m$}
			\State $sum \gets sum + A_{ik} \; B_{kj}$
		\EndFor
		\State $C_{ij} \gets sum$
	\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

Questo algoritmo richiede un tempo pari a $O(nmp)$ (se le matrici sono quadrate $n\times n$ il tempo diventa $O(n^3)$).

\paragraph{Cache optimization:} I tre cicli all'interno della moltiplicazione possono essere scambiati senza modificare il risultato in termini matematici, tuttavia questo può avere un considerevole impatto in termini di implementazione ed in particolare in termini di tempo di esecuzione su elaboratori elettronici. Infatti i computer hanno degli algoritmi ottimizzati per l'accesso dei dati dalla RAM alla CPU quando questi sono sequenziali (uno di fila all'altro in RAM). Quando il dato successivo necessario all'algoritmo non è quello immediatamente successivo in RAM si parla di \textit{cache miss}.
La versione ottimale dell'algoritmo semplice per due matrici $\mathbf{A}$ e $\mathbf{B}$ in row-major è la versione tiled, dove le matrici sono implicitamente divise in \textit{tiles} di dimensione $\sqrt{N} \times \sqrt{N}$, è descritta nell'algoritmo~\ref{alg:mul_simple_cache}.

\begin{algorithm}
\caption{Semplice algoritmo per la moltiplicazione tra matrici con gestione migliorata della cache \textcolor{red}{non mi torna un granche`}}\label{alg:mul_simple_cache}
\begin{algorithmic}[1]
\Require matrix $\mathbf{A}$, matrix $\mathbf{B}$
\State $\mathbf{C} \gets $ matrix of the appripriate size
\State tile size $T \gets \sqrt{N}$
\For{$I \gets 1$ to $n$ in steps of $T$}
	\For{$J \gets 1$ to $p$ in steps of $T$}
		\For{$K \gets 1$ to $m$ in steps of $T$}
			\For{$i \gets I$ to $\min(I+T,n)$} \Comment{Multiply $\mathbf{A}_{I:I+T, K:K+T}$ and $\mathbf{B}_{K:K+T, J:J+T}$ into $\mathbf{C}_{I:I+T, J:J+T}$}
				\For{$j \gets J$ to $\min(J+T,p)$}
					\State $sum \gets 0$
					\For{$k \gets K$ to $\min(K+T,m)$}
						\State $sum \gets sum + A_{ik} \; B_{kj}$
					\EndFor
					\State $C_{ij} \gets C_{ij} + sum$
				\EndFor
			\EndFor
		\EndFor
	\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

Nel modello di cache ideale l'algoritmo incorre in solamente $O(\frac{n^3}{b\sqrt{N}}$. Per le macchine moderne il denominatore $m\sqrt{N}$ ammonta a diversi ordini di grandezza, dunque il tempo necessario è quello effettivo del calcolo, invece di perdere tempo in \textit{cache misses}.


\paragraph{Divide-and-conquer algorithm:} Una alternativa è l'algoritmo \textit{dividi e conquista} per la moltiplicazione delle matrici che fa affidamento nella partizione delle matrici in blocchi
\begin{equation}
\begin{bmatrix}
\mathbf{C}_{11} & \mathbf{C}_{12} \\
\mathbf{C}_{12} & \mathbf{C}_{22}
\end{bmatrix} = 
\begin{bmatrix}
\mathbf{A}_{11} & \mathbf{A}_{12} \\
\mathbf{A}_{12} & \mathbf{A}_{22}
\end{bmatrix}
\begin{bmatrix}
\mathbf{B}_{11} & \mathbf{B}_{12} \\
\mathbf{B}_{12} & \mathbf{B}_{22}
\end{bmatrix} = 
\begin{bmatrix}
\mathbf{A}_{11}\;\mathbf{B}_{11}+\mathbf{A}_{12}\;\mathbf{B}_{21} & \mathbf{A}_{11}\;\mathbf{B}_{12}+\mathbf{A}_{12}\;\mathbf{B}_{22} \\
\mathbf{A}_{21}\;\mathbf{B}_{11}+\mathbf{A}_{22}\;\mathbf{B}_{21} & \mathbf{A}_{21}\;\mathbf{B}_{12}+\mathbf{A}_{22}\;\mathbf{B}_{22}
\end{bmatrix}
\end{equation}
che si applica a tutte le matrici quadrate che hanno dimensione pari a una potenza di due (${2^n \times 2^n}$).
L'algoritmo dividi e conquista calcola le moltiplicazioni più piccole ricorsivamente come un prodotto scalare $C_{11} = A_{11}B_{11}$ come caso base.

Una variante che funziona per matrici non-quadrate si basa sul dividere le matrici in due invece che in quattro. In questo caso si tratta di dividere le matrici in due parti uguali, o comunque il più vicino possibile a due parti uguali nel caso di dimensioni dispari.

\begin{algorithm}
\caption{Moltiplicazione tra metrici tramite \textit{divide-and-conquer}}\label{alg:mul_div_conq}
\begin{algorithmic}[1]
\Require matrix $\mathbf{A} \in \mathbb{R}^{n\times m}$, matrix $\mathbf{B} \in \mathbb{R}^{m\times p}$
\State if $\max(n,m,p)$ is below some threshold, use an unrolled version of the simple algorithm
\If{$\max(n,m,p) = n$}
	\State $\mathbf{C} = \begin{bmatrix} \mathbf{A}_1 \\ \mathbf{A}_2 \end{bmatrix}\mathbf{B} = \begin{bmatrix} \mathbf{A}_1 \mathbf{B} \\ \mathbf{A}_2 \mathbf{B} \end{bmatrix}$ \Comment{split $\mathbf{A}$ horizontally}
\\\textbf{else if} $\max(n,m,p) = p$
	\State $\mathbf{C} = \mathbf{A}\begin{bmatrix} \mathbf{B}_1 & \mathbf{B}_2 \end{bmatrix} = \begin{bmatrix} \mathbf{A} \mathbf{B}_1 \\ \mathbf{A} \mathbf{B}_2 \end{bmatrix}$ \Comment{split $\mathbf{B}$ vertically}
\Else \Comment($\max(n,m,p) = m$)
	\State $\mathbf{C} = \begin{bmatrix} \mathbf{A}_1 & \mathbf{A}_2 \end{bmatrix}\begin{bmatrix} \mathbf{B}_1 \\ \mathbf{B}_2 \end{bmatrix} = \mathbf{A}_1 \mathbf{B}_1 + \mathbf{A}_2 \mathbf{B}_2$ \Comment{split $\mathbf{A}$ vertically and $\mathbf{B}$ horizontally}
\EndIf
\end{algorithmic}
\end{algorithm}






\section{Gauss elimination}

L'eliminazione di Gauss consiste nell'eseguire operazioni sulla matrice che non cambiano il risultato del sistema di equazioni associato, come ad esempio \textit{scambiare delle righe, moltiplicare righe per scalari e sommare una riga all'altra}. Queste operazioni sono eseguite per semplificare il problema.

L'eliminazione di Gauss viene usata per portare la matrice ad una forma più facile da gestire, chiamata \textit{Row Echelon Form} (REF). In questa forma la matrice ha le seguenti proprietà:
\begin{itemize}[noitemsep]
\item il primo\footnote{Primo elemento non nullo della riga partendo da sinistra verso destra.} elemento non nullo di una riga (\emph{pivot}) è esattamente 1.0
\item righe che hanno tutti elementi nulli sono sotto le righe che hanno almeno un elemento non nullo
\item ogni pivot di una riga si trova ad una colonna alla destra dei pivot delle righe superiori
\end{itemize}
L'algoritmo che trasforma la matrice nella \textit{Row Echelon Form} è il seguente:
\begin{enumerate}[noitemsep]
\item Trova il pivot (primo elemento non nullo della riga). Se la colonna ha elementi tutti nulli passa alla colonna successiva.
\item Scambia le righe posizionando la riga del pivot come prima.
\item Moltiplica ogni elemento sulla riga per $\frac{1}{\text{pivot}}$ in maniera tale che il pivot è pari a 1.0.
\item Tramite la somma tra righe (premoltiplicando per il pivot dell'altra riga), fa in maniera tale che tutti gli elementi sulla colonna del pivot (elementi che non sono il pivot) sono pari a zero.
\item Continua il processo finché non si è giunti alla \textit{Row Echelon Form}.
\end{enumerate}

La forma \textit{Reduced Row Echelon Form} (RREF) consiste nell'avere solamente un valore diverso da zero per ogni colonna.
Si nota che una matrice può avere molte REF, ma solamente una RREF.


\subsection{Basic operations}

Le operazioni base dell'eliminazione di Gauss sono:
\begin{itemize}[noitemsep]
\item Scambio di due righe (row swapping: \verb|matd_row_swap_r|).
\item Moltiplica ogni elemento della riga per uno scalare diverso da zero (scalar multiplication of rows: \verb|matd_row_smul_r|).
\item Moltiplica una riga per in termine diverso da zero e aggiunge il risultato a un'altra riga (row addition: \verb|matd_row_addrow_r|).
\end{itemize}

\subsubsection{Row swapping}

\begin{lstlisting}
int matd_row_swap_r(matd *m, unsigned int row1, unsigned int row2)
{
	// swap two rows of matrix m
	int i;
	double tmp;
	if(row1 >= m->n_rows || row2 >= m->n_rows) return 0; // cannot swap rows
	for(i=0; i<m->n_cols; i++){
		tmp = m->data[row2*m->n_cols+i];
		m->data[row2*m->n_cols+i] = m->data[row1*m->n_cols+i];
		m->data[row1*m->n_cols+i] = tmp;
	}
	return 1;
}
\end{lstlisting}

Esempio:
\begin{equation*}
\mathtt{matd\_row\_swap\_r} = \left( 
\begin{bmatrix}
1 & 2 & 3 \\ 0 & 2 & 4 \\ 2 & 1 & 9
\end{bmatrix}, \; 0, \; 1
\right) = 
\begin{bmatrix}
0 & 2 & 4 \\ 1 & 2 & 3 \\ 2 & 1 & 9
\end{bmatrix}
\end{equation*}

\subsubsection{Scalar multiplication of rows}

\begin{lstlisting}
int matd_row_smul_r(matd *m, unsigned int row, double num)
{
	int i;
	if(row >= m->n_rows) return 0; // cannot row multiply
	for(i=0; i<m->n_cols; i++) m->data[row*m->n_cols+i] *= num;
	return 1;
}
\end{lstlisting}

Esempio:
\begin{equation*}
\mathtt{matd\_row\_swap\_r} = \left( 
\begin{bmatrix}
1 & 2 & 3 \\ 0 & 2 & 4 \\ 2 & 1 & 9
\end{bmatrix}, \; 1, \; 2.0
\right) = 
\begin{bmatrix}
1 & 2 & 3 \\ 0.0 & 4.0 & 6.0 \\ 2 & 1 & 9
\end{bmatrix}
\end{equation*}

\subsubsection{Row addition}

\begin{lstlisting}
int matd_row_addrow_r(matd *m, unsigned int where, unsigned int row, double multiplier)
{
	int i = 0;
	if(where >= m->n_rows || row >= m->n_rows) return 0; // cannot add rows
	for(i=0; i<m->n_cols; i++) m->data[where*m->n_cols+i] += multiplier * m->data[row*m->n_cols+i];
	return 1;
}
\end{lstlisting}

Esempio:
\begin{equation*}
\mathtt{matd\_row\_addrow\_r} = \left( 
\begin{bmatrix}
1 & 2 & 3 \\ 0 & 2 & 4 \\ 2 & 1 & 9
\end{bmatrix}, \; 0, \; 1, \; 0.5
\right) = 
\begin{bmatrix}
1+0\cdot 0.5 & 2+2\cdot 0.5 & 3+4\cdot 0.5 \\
0 & 2 & 4 \\
4 & 1 & 9
\end{bmatrix} = 
\begin{bmatrix}
1 & 3 & 5 \\ 0 & 2 & 4 \\ 2 & 1 & 9
\end{bmatrix}
\end{equation*}



\section{Solve linear systems}

Risolvere il sistema di equazioni lineari:
\begin{equation}
\mathbf{A} \; \mathbf{x} = \mathbf{b}
\label{eq:lin_sys}
\end{equation}
vuol dire trovare il vettore colonna incognito $\mathbf{x}$ sapendo la matrice $\mathbf{A}$ e il vettore dei termini noti $\mathbf{b}$. La soluzione è trovata in maniera esatta tramite la matrice inversa di $\mathbf{A}$:
\begin{equation}
\mathbf{x} = \mathbf{A}^{-1} \; \mathbf{b}
\end{equation}
Dal punto di vista dell'implementazione il principale problema risiede nel fatto che calcolare la matrice inversa$\mathbf{A}^{-1}$ attraverso il metodo di \textcolor{red}{Come si chiamava? (quello lentissimo esatto)} è un'operazione molto dispendiosa.
Per questo esistono molti metodi per calcolare velocemente il vettore ignoto $\mathbf{x}$ in maniera computazionalmente efficiente.


\subsection{Gauss-Jordan elimination}


\subsection{LU(P) decomposition}

La decomposizione LU, chiamata anche la fattorializzazione LU, si riferisce alla scomposizione di una matrice $\mathbf{A}$ in due matrici $\mathbf{L}$ e $\mathbf{U}$:
\begin{equation}
\mathbf{A} = \mathbf{L} \; \mathbf{U}
\qquad \rightarrow \qquad
\begin{bmatrix}
A_{11} & A_{12} & A_{13} \\
A_{21} & A_{22} & A_{23} \\
A_{31} & A_{32} & A_{33}
\end{bmatrix} = 
\begin{bmatrix}
1 & 0 & 0 \\
L_{21} & 1 & 0 \\
L_{31} & L_{32} & 1
\end{bmatrix}
\begin{bmatrix}
U_{11} & U_{12} & U_{13} \\
0      & U_{22} & U_{23} \\
0      & 0      & U_{33}
\end{bmatrix}
\end{equation}

In pratica questa fattorializzazione viene ricavata da operazioni elementari sulle righe (come lo scambio delle righe). In tal caso bisogna introdurre nell'equazione una matrice di permutazione $\mathbf{P}$ che tiene conto del cambio delle righe:
\begin{equation}
\mathbf{P} \; \mathbf{A} = \mathbf{L} \; \mathbf{U}
\label{eq:LUp}
\end{equation}
dove $\mathbf{P}$ è una matrice ricavata dalla permutazione delle righe di una matrice identità $\mathbf{I}$, ed è calcolata dalla procedura che trova $\mathbf{L}$ e $\mathbf{U}$.
Se una matrice $\mathbf{A}$ é quadrata (di dimensione $n\times n$) può sempre essere scomposta come $\mathbf{P} \; \mathbf{A} = \mathbf{L} \; \mathbf{U}$.
Per trovare la decomposizione LU si usa versione modificata dell'algoritmo per l'eliminazione di Gauss. Questa implementazione richiede circa $\frac{2}{3}n^3$ operazioni a virgola mobile. Altri algoritmi richiedono funzioni ricorsive o randomizzazione.

Trovare la decomposizione \eqref{eq:LUp} permette di calcolare il determinante della matrice $\mathbf{A}$, la sua inversa e, quindi, anche risolvere i sistemi di equazioni lineari.

Esiste anche un'altra fattorializzazione dove non solo le righe, ma anche le colonne vengono considerate e si chiama \emph{LU Factorialization with full pivoting}.

\textcolor{red}{Descrizione dell'algoritmo e codice per la soluzione tramite decomposizione LU}

Usando la decomposizione \eqref{eq:LUp} il sistema di equazioni \eqref{eq:lin_sys} diventa:
\begin{equation}
\mathbf{A} \; \mathbf{x} = \mathbf{b}
\qquad \rightarrow \qquad
\mathbf{P} \; \mathbf{A} \; \mathbf{x} = \mathbf{P} \; \mathbf{b}
\qquad \rightarrow \qquad
\mathbf{L}\ \; \mathbf{U} \; \mathbf{x} = \mathbf{P} \; \mathbf{b}
\end{equation}
Introducendo il sistema di equazioni ausiliario $\mathbf{y} = \mathbf{U} \; \mathbf{x}$ troviamo la soluzione $\mathbf{x}$ del sistema lineare \eqref{eq:lin_sys} risolvendo i due sistemi:
\begin{align}
\mathbf{y} & = \mathbf{U} \; \mathbf{x} \\
\mathbf{L} \; \mathbf{y} & = \mathbf{P} \; \mathbf{b}
\end{align}
Per risolvere questi due sistemi di equazioni lineari si procede a trovare il vettore temporaneo $\mathbf{y}$ tramite sostituzione in avanti (\textit{forward substitution}) del sistema $\mathbf{L} \; \mathbf{y} = \mathbf{P} \; \mathbf{b}$, dato che $\mathbf{L}$ è una matrice triangolare inferiore. Successivamente si trova la soluzione $\mathbf{x}$ tramite il sistema $\mathbf{U} \; \mathbf{x} = \mathbf{y}$ tramite sostituzione all'indietro\footnote{All'indietro, nel senso che parte trovando l'ultimo termine di $\mathbf{x}$ e va all'indietro fino al primo $x_1$. Il contrario rispetto alla \textit{forward substitution}.} (\textit{backward substitution}), dato che la matrice $\mathbf{U}$ è triangolare superiore.

\begin{lstlisting}
matd *lu_solve(matd_lup *lu, matd* b)
{
	matd *Pb, *x, *y;
	if(lu->U->n_rows != b->n_rows || b->n_cols != 1) return NULL;
	Pb = matd_mul(lu->P, b);
	
	y = lu_solvefwd(lu->L, Pb); // Solve L*y = P*b using forward substition
	x = lu_solvebck(lu->U, y); // Solve U*x=y using backward substitution
	
	free_mat(y); // free memory space of the temporary matrices	
	free_mat(Pb);
	return x;
}
\end{lstlisting}




\subsection{QR decomposition}

Qualsiasi matrice simmetrica $\mathbf{A}$ può essere decomposta come:
\begin{equation}
\mathbf{A} = \mathbf{Q} \; \mathbf{R}
\label{eq:QR}
\end{equation}
dove $\mathbf{Q}$ è una matrice ortogonale\footnote{Una matrice si dice \emph{ortogonale} se
\begin{equation}
\mathbf{Q}\;\mathbf{Q}^T = \mathbf{Q}^T\mathbf{Q} = \mathbf{I}
\end{equation}
Questo equivale a dire che la trasposta della matrice è uguale all'inversa ($\mathbf{Q}^T=\mathbf{Q}^{-1}$).} e $\mathbf{R}$ una matrice triangolare superiore.
Per trovare la decomposizione \eqref{eq:QR} si usa l'algoritmo di Gram-Schmidt.

Al contrario della fattorializzaizone LU(P) che si concentra sulle operazioni sulle righe, la decomposizione QR utilizza operazioni sulle colonne.
Considerando le matrici (in questo caso di dimensione $3\times 3$):
\begin{equation}
\mathbf{A} = 
\begin{bmatrix}
| & | & | \\
\mathbf{a}_1 & \mathbf{a}_2 & \mathbf{a}_3 \\
| & | & |
\end{bmatrix}
\qquad \qquad
\mathbf{Q} = 
\begin{bmatrix}
| & | & | \\
\mathbf{q}_1 & \mathbf{q}_2 & \mathbf{q}_3 \\
| & | & |
\end{bmatrix}
\end{equation}
dove $\mathbf{a}_i$ sono i vettori colonna che formano la matrice $\mathbf{A}$ e $\mathbf{q}_i$ i vettori colonna di $\mathbf{B}$.
La struttura della matrice $\mathbf{R}$ deriva dal fatto che $\mathbf{Q}$, essendo una matrice ortogonale, ha per colonne dei vettori unitari, ovvero di norma euclidea unitaria.
Questo porta alle seguenti formule:
\begin{equation}
\begin{dcases}
\mathbf{q}_1 = \frac{\mathbf{a}_1}{||\mathbf{a}_1||} \\
\mathbf{q}_2 = \frac{\mathbf{a}_2^\perp}{||\mathbf{a}_2^\perp||} \\
\mathbf{q}_2 = \frac{\mathbf{a}_3^\perp}{||\mathbf{a}_3^\perp||}
\end{dcases}
\qquad\qquad \text{dove} \qquad
\begin{array}{l}
\mathbf{a}_2^\perp  = \mathbf{a}_2 - \left<\mathbf{a}_2,\mathbf{q}_1\right> \mathbf{q}_1 \\ \\
\mathbf{a}_3^\perp  = \mathbf{a}_3 - \left<\mathbf{a}_3,\mathbf{q}_1\right> \mathbf{q}_1 - \left<\mathbf{a}_3,\mathbf{q}_2\right> \mathbf{q}_2
\end{array}
\end{equation}
dove $\left<\mathbf{u},\mathbf{v}\right>$ è il prodotto scalare tra i due vettori $\mathbf{u}$ e $\mathbf{v}$, mentre $||\mathbf{u}||$ è la norma-2 euclidea.
Quindi la decomposizione QR di questo esempio per matrici $3\times 3$ è pari a:
\begin{equation}
\mathbf{A} = 
\begin{bmatrix}
| & | & | \\
\mathbf{a}_1 & \mathbf{a}_2 & \mathbf{a}_3 \\
| & | & |
\end{bmatrix} = 
\begin{bmatrix}
| & | & | \\
\frac{\mathbf{a}_1}{||\mathbf{a}_1||} & \frac{\mathbf{a}_2^\perp}{||\mathbf{a}_2^\perp||} & \frac{\mathbf{a}_3^\perp}{||\mathbf{a}_3^\perp||} \\
| & | & |
\end{bmatrix}
\begin{bmatrix}
||\mathbf{a}_1|| & \left<\mathbf{a}_2,\mathbf{q}_1\right> & \left<\mathbf{a}_3,\mathbf{q}_1\right> \\
0 & ||\mathbf{a}_2^\perp|| & \left<\mathbf{a}_3,\mathbf{q}_2\right> \\
0 & 0 & ||\mathbf{a}_3^\perp||
\end{bmatrix}
\end{equation}
Questa formula può essere generalizzata al caso $n\times n$.
















\newpage

\appendix

\section{Random numbers}

\textcolor{red}{Algoritmi di generazione dei numeri casuali}

\subsection{Sampling from Gaussian distribution}

\textcolor{red}{metodi di sampling descritti sul mio quaderno dei ponti integrali, con qualche indicazione in piu`}



\end{document}
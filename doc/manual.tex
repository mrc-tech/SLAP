\documentclass[10pt]{article}

\usepackage[a4paper,top=2.5cm,bottom=3.5cm,left=2.5cm,right=2.5cm]{geometry}
%\usepackage[T1]{fontenc}		%implementa nei font gli accenti (crea un casino facendo tutto pixellato)
\usepackage[utf8]{inputenc}		%importa i caratteri utf
\usepackage{inputenc}
\usepackage[italian]{babel}
\usepackage{amsmath,amssymb}	%per visualizzare la matematica (per text{})
\usepackage{mathtools}
%\usepackage{graphicx}			%per le figure
\usepackage{xcolor}				%per scrivere colorato XCOLOR per lstlisting colorato (se non c'è tikz)
\usepackage[toc,page]{appendix}	%per l'implementazione delle appendici
%\usepackage{subfig}				%per figure multiple in una stessa figure
%\usepackage{pdfpages}			%per includere pezzi di pdf
%\usepackage{float}				%per mettere le figure nel posto giusto
\usepackage{tikz}				%per le immagini vettoriali da LateX
\usepackage{listings} 			%Per inserire codice
\usepackage{pxfonts}			%per il bold nei lstlisting
%\usepackage{scrextend}			%per labeling
\usepackage{enumitem}			% per [noitemsep]
\usepackage{algorithm}			% per la descrizione degli algoritmi
\usepackage{algpseudocode}		% per lo pseudocodice


%\definecolor{mymauve}{rgb}{0.58,0,0.82}

\lstset { %
	language=C++,
	backgroundcolor=\color{black!5}, % set backgroundcolor
	basicstyle=\footnotesize\ttfamily, % basic font setting
	frame=shadowbox,
	breaklines= true, % va a capo automatico
	numbers=left, % numera le righe
	commentstyle=\color{gray}, % commenti grigi
	stringstyle=\color{mymauve}, % string literal style
	numberstyle=\footnotesize\ttfamily, %formato dei numeri a lato delle righe
	keywordstyle=\color{black}\bfseries, % style for keywords
}




\begin{document}


\title{\textbf{SLAP}\\ Simple Linear Algebra Package}
\author{Andrea Marchi}

\maketitle

\tableofcontents



\section{Data type}

Le matrici sono salvate in array monodimensionali \textit{row-major}, ovvero che i dati sulle righe sono sequenziali. Quindi nella indicizzazione degli elementi della matrice con $n$ righe e $m$ colonne ($\mathbb{R}^{n\times m}$) si usa la formula:
\begin{lstlisting}
matrix[i][j] = array[i*m + j]
\end{lstlisting}
Le righe vanno da $0$ a $n-1$, mentre le colonne vanno da $0$ a $m-1$.

Alcune librerie usano la notazione \textit{column-major}, ovvero con indicizzazione \verb|array[j*n+i]|. Avere una funzione che organizza i dati della matrice in colonne o in righe è comodo per quanto riguarda l'ottimizzazione della cache di lettura dei dati sequenziali dalla RAM alla CPU.

Il tipo di dati (essendo in C) è una struttura (\verb|struct|) e la definizione cambia al variare del tipo di dati base (il C non permette l'uso di template).
La struttura base è:
\begin{lstlisting}
typedef struct _matd{
	unsigned int n_rows;
	unsigned int n_cols;
	double *data; // row-major matrix data array
} matd;
\end{lstlisting}
dove la lettera finale indica il tipo di variabile usata, in questo caso \verb|double|. I tipi di dati che ha senso utilizzare nella libreria sono:
\begin{description}[noitemsep]
\item[\texttt{d}] virgola mobile a doppia precisione (\verb|double|)
\item[\texttt{f}] virgola mobile (\verb|float|)
\item[\texttt{i}] intero (\verb|int|)
\item[\texttt{b}] byte (\verb|unsigned char|)
\end{description}

Per allocare la memoria e liberarla (sempre liberare la memoria dopo averla allocata):
\begin{lstlisting}
matd* new_matd(unsigned int num_rows, unsigned int num_cols)
{
	int i;
	// create a new double matrix
	if(num_rows == 0) { /*SLAP_ERROR(INVALID_ROWS);*/ return 0; } // dovrebbe ritornare NULL
	if(num_cols == 0) { /*SLAP_ERROR(INVALID_COLS);*/ return 0; } // dovrebbe ritornare NULL
	
	matd *m = calloc(1, sizeof(*m)); // allocate space for the struct
	// CONTROLLARE LA MATRICE CREATA ( SLAP_CHECK(m) )
	m->n_rows = num_rows;
	m->n_cols = num_cols;
	m->data = calloc(m->n_rows*m->n_cols, sizeof(*m->data));
	// CONTROLLARE I DATI CREATI ( SLAP_CHECK(m->data) )
	for(i=0; i<num_rows*num_cols; i++) m->data[i] = 0; // set to zero
	
	return m;
}

void free_mat(matd *matrix)
{
	free(matrix->data); // delete the data
	free(matrix); // delete the data structure
}
\end{lstlisting}

Come setters e getters non potendo usare le operation del C++ e non riuscendo a fare qualcosa di funzionante e decente con le macro\footnote{Usare le macro mi permetterebbe di risparmiare tempo nella allocazione dei parametri delle funzioni. Negli algoritmi potrei usare l'accesso diretto all'array \texttt{data}.} uso le funzioni
\begin{lstlisting}
double matd_get(matd matrix, unsigned int row, unsigned int col) {
	return matrix.data[row*matrix.n_cols + col]; // row-major
}
void matd_set(matd matrix, unsigned int row, unsigned int col, double val) {
	matrix.data[row*matrix.n_cols + col] = val; // row-major
}
\end{lstlisting}
\textcolor{red}{Dovrei controllare che l'accesso sia corretto (che non cerchi di scrivere/leggere dati non allocati).}



\section{Basic operations}

\textcolor{red}{Mettere un pannello (anche una tabella) che riassume le operazioni, il nome delle funzioni e come si usano.}

\subsection{Equality}

\subsection{Addition and sottration}

\subsection{Multiplication}

\subsubsection{Scalar-matrix multiplication}
\subsubsection{Vector-matrix multiplication}

La moltiplicazione matrice-vettore può essere vista come la moltiplicazione tra una matrice e un'altra nella forma di un vettore colonna.
\textcolor{red}{Fare un controllo numerico su questa affermazione}

\subsubsection{Matrix-matrix multiplication}

Il prodotto tra due matrici $\mathbf{A} \in \mathbb{R}^{n\times m}$ (matrice con $n$ righe e $m$ colonne) $\mathbf{B} \in \mathbb{R}^{m\times p}$ ($m$ righe, $p$ colonne) è una matrice $\mathbf{C} \in \mathbb{R}^{n\times p}$ le cui componenti sono definite come:
\begin{equation}
C_{ij} = \sum_{k=1}^m A_{ik} \; B_{kj}
\end{equation}

Applicando direttamente la definizione della moltiplicazione tra matrici si ottiene un algoritmo che ha un'efficienza computazionale di $O(n^3)$ (dove $n$ è la dimensione di una matrice quadrata $n\times n$). Migliori limiti asintotici sono stati scoperti dopo l'algoritmo di Strassen negli anni '60 (il limite teorico rimane ancora ignoto). Recentemente è stato annunciato un algoritmo con l'efficienza teorica di $O(n^{2.37188})$, ma si tratta di un \emph{algoritmo galattico}, ovvero che richiede una dimensione talmente grande delle matrici per funzionare con tale efficienza da non essere praticamente realizzabile (in questo caso a causa di una grossa costante).

\paragraph{Simple algorithm:} Dalla definizione della moltiplicazione tra matrici si ricava direttamente l'algoritmo~\ref{alg:mul_simple}.

\begin{algorithm}
\caption{Semplice algoritmo per la moltiplicazione tra matrici}\label{alg:mul_simple}
\begin{algorithmic}[1]
\Require matrix $\mathbf{A}$, matrix $\mathbf{B}$
\State $\mathbf{C} \gets $ matrix of the appripriate size
\For{$i \gets 1$ to $n$} \Comment{nell'implementazione parte da $0$ e arriva a $n-1$}
	\For{$j \gets 1$ to $p$} \Comment{stessa considerazione dell'indice $i$}
		\State $sum \gets 0$
		\For{$k \gets 1$ to $m$}
			\State $sum \gets sum + A_{ik} \; B_{kj}$
		\EndFor
		\State $C_{ij} \gets sum$
	\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

Questo algoritmo richiede un tempo pari a $O(nmp)$ (se le matrici sono quadrate $n\times n$ il tempo diventa $O(n^3)$).

\paragraph{Cache optimization:} I tre cicli all'interno della moltiplicazione possono essere scambiati senza modificare il risultato in termini matematici, tuttavia questo può avere un considerevole impatto in termini di implementazione ed in particolare in termini di tempo di esecuzione su elaboratori elettronici. Infatti i computer hanno degli algoritmi ottimizzati per l'accesso dei dati dalla RAM alla CPU quando questi sono sequenziali (uno di fila all'altro in RAM). Quando il dato successivo necessario all'algoritmo non è quello immediatamente successivo in RAM si parla di \textit{cache miss}.
La versione ottimale dell'algoritmo semplice per due matrici $\mathbf{A}$ e $\mathbf{B}$ in row-major è la versione tiled, dove le matrici sono implicitamente divise in \textit{tiles} di dimensione $\sqrt{N} \times \sqrt{N}$, è descritta nell'algoritmo~\ref{alg:mul_simple_cache}.

\begin{algorithm}
\caption{Semplice algoritmo per la moltiplicazione tra matrici con gestione migliorata della cache \textcolor{red}{non mi torna un granche`}}\label{alg:mul_simple_cache}
\begin{algorithmic}[1]
\Require matrix $\mathbf{A}$, matrix $\mathbf{B}$
\State $\mathbf{C} \gets $ matrix of the appripriate size
\State tile size $T \gets \sqrt{N}$
\For{$I \gets 1$ to $n$ in steps of $T$}
	\For{$J \gets 1$ to $p$ in steps of $T$}
		\For{$K \gets 1$ to $m$ in steps of $T$}
			\For{$i \gets I$ to $\min(I+T,n)$} \Comment{Multiply $\mathbf{A}_{I:I+T, K:K+T}$ and $\mathbf{B}_{K:K+T, J:J+T}$ into $\mathbf{C}_{I:I+T, J:J+T}$}
				\For{$j \gets J$ to $\min(J+T,p)$}
					\State $sum \gets 0$
					\For{$k \gets K$ to $\min(K+T,m)$}
						\State $sum \gets sum + A_{ik} \; B_{kj}$
					\EndFor
					\State $C_{ij} \gets C_{ij} + sum$
				\EndFor
			\EndFor
		\EndFor
	\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

Nel modello di cache ideale l'algoritmo incorre in solamente $O(\frac{n^3}{b\sqrt{N}}$. Per le macchine moderne il denominatore $m\sqrt{N}$ ammonta a diversi ordini di grandezza, dunque il tempo necessario è quello effettivo del calcolo, invece di perdere tempo in \textit{cache misses}.


\paragraph{Divide-and-conquer algorithm:} Una alternativa è l'algoritmo \textit{dividi e conquista} per la moltiplicazione delle matrici che fa affidamento nella partizione delle matrici in blocchi
\begin{equation}
\begin{bmatrix}
\mathbf{C}_{11} & \mathbf{C}_{12} \\
\mathbf{C}_{12} & \mathbf{C}_{22}
\end{bmatrix} = 
\begin{bmatrix}
\mathbf{A}_{11} & \mathbf{A}_{12} \\
\mathbf{A}_{12} & \mathbf{A}_{22}
\end{bmatrix}
\begin{bmatrix}
\mathbf{B}_{11} & \mathbf{B}_{12} \\
\mathbf{B}_{12} & \mathbf{B}_{22}
\end{bmatrix} = 
\begin{bmatrix}
\mathbf{A}_{11}\;\mathbf{B}_{11}+\mathbf{A}_{12}\;\mathbf{B}_{21} & \mathbf{A}_{11}\;\mathbf{B}_{12}+\mathbf{A}_{12}\;\mathbf{B}_{22} \\
\mathbf{A}_{21}\;\mathbf{B}_{11}+\mathbf{A}_{22}\;\mathbf{B}_{21} & \mathbf{A}_{21}\;\mathbf{B}_{12}+\mathbf{A}_{22}\;\mathbf{B}_{22}
\end{bmatrix}
\end{equation}
che si applica a tutte le matrici quadrate che hanno dimensione pari a una potenza di due (${2^n \times 2^n}$).
L'algoritmo dividi e conquista calcola le moltiplicazioni più piccole ricorsivamente come un prodotto scalare $C_{11} = A_{11}B_{11}$ come caso base.

Una variante che funziona per matrici non-quadrate si basa sul dividere le matrici in due invece che in quattro. In questo caso si tratta di dividere le matrici in due parti uguali, o comunque il più vicino possibile a due parti uguali nel caso di dimensioni dispari.

\begin{algorithm}
\caption{Moltiplicazione tra metrici tramite \textit{divide-and-conquer}}\label{alg:mul_div_conq}
\begin{algorithmic}[1]
\Require matrix $\mathbf{A} \in \mathbb{R}^{n\times m}$, matrix $\mathbf{B} \in \mathbb{R}^{m\times p}$
\State if $\max(n,m,p)$ is below some threshold, use an unrolled version of the simple algorithm
\If{$\max(n,m,p) = n$}
	\State $\mathbf{C} = \begin{bmatrix} \mathbf{A}_1 \\ \mathbf{A}_2 \end{bmatrix}\mathbf{B} = \begin{bmatrix} \mathbf{A}_1 \mathbf{B} \\ \mathbf{A}_2 \mathbf{B} \end{bmatrix}$ \Comment{split $\mathbf{A}$ horizontally}
\\\textbf{else if} $\max(n,m,p) = p$
	\State $\mathbf{C} = \mathbf{A}\begin{bmatrix} \mathbf{B}_1 & \mathbf{B}_2 \end{bmatrix} = \begin{bmatrix} \mathbf{A} \mathbf{B}_1 \\ \mathbf{A} \mathbf{B}_2 \end{bmatrix}$ \Comment{split $\mathbf{B}$ vertically}
\Else \Comment($\max(n,m,p) = m$)
	\State $\mathbf{C} = \begin{bmatrix} \mathbf{A}_1 & \mathbf{A}_2 \end{bmatrix}\begin{bmatrix} \mathbf{B}_1 \\ \mathbf{B}_2 \end{bmatrix} = \mathbf{A}_1 \mathbf{B}_1 + \mathbf{A}_2 \mathbf{B}_2$ \Comment{split $\mathbf{A}$ vertically and $\mathbf{B}$ horizontally}
\EndIf
\end{algorithmic}
\end{algorithm}


\paragraph{Strassen algorithm}

L'osservazione chiave è che per matrici $2\times 2$ le moltiplicazioni necessarie sono 7 invece che 8, al costo di 11 operazioni aggiuntive tra somme e sottrazioni. Quindi si tratta di un algoritmo \textit{dividi e conquista} in cui la matrici di input sono suddivise in blocchi $2\times 2$ e quindi bisogna moltiplicare sette $\frac{n}{2}\times\frac{n}{2}$ matrici. Applicando ricorsivamente l'algoritmo richiede in totale $O(n^{\log_27}\approx O(n^{2.807})$ operazioni sul campo.

La stabilità è inferiore rispetto all'algoritmo naive, ma più veloce nel caso in cui $n>100$ circa.

\textcolor{red}{continuare a scrivere...}




\section{Functions}

\subsection{Matrix norm}

La norma di una matrice è una funzione $||\cdot|| : \mathbb{K}^{n\times m} \mapsto \mathbb{K}$ che soddisfa le seguenti proprietà:
\begin{itemize}[noitemsep]
\item $||\mathbf{A}|| \geq 0$ con l'uguaglianza solamente se $\mathbf{A}=\mathbf{0}$ (sempre positiva)
\item $||\alpha\mathbf{A}||=|\alpha| \; ||\mathbf{A}||$ per ogni $\alpha\in\mathbb{K}$ (omogeneità)
\item $||\mathbf{A}+\mathbf{B}||\leq ||\mathbf{A}||+||\mathbf{B}||$ (disuguaglianza triangolare)
\end{itemize}
Si tratta di un operatore analogo alla norma di un vettore, ma applicata alle matrici.

Un'importante classe di norme matriciali è data dalla \textit{subordinate matrix norm} definita a partire dalla norma di un vettore:
\begin{equation}
||\mathbf{A}||_p = \max_{\mathbf{x}\neq\mathbf{0}} \frac{||\mathbf{A}\mathbf{x}||_p}{||\mathbf{x}||_p}
\end{equation}
che equivalentemente può essere scritta come $||\mathbf{A}||_p = \max_{||\mathbf{x}||_p=1} ||\mathbf{A}\mathbf{x}||_p$.
Per le norme 1--, 2-- e $\infty$-- del vettore si ha che:
\begin{align}
||\mathbf{A}||_1 & = \max_{1\leq j\leq m} \sum_{i=1}^n |A_{ij}| \\
||\mathbf{A}||_2 & = \max_{1\leq i\leq n} \sum_{j=1}^m |A_{ij}| \\
||\mathbf{A}||_\infty & = \sigma_{\max}(\mathbf{A})
\end{align}
dove $\sigma_{\max}(\mathbf{A})$ denota il valore singolare maggiore di $\mathbf{A}$. per la forma generale della norma $p$-esima di H\"older non è presente una formula esplicita per i casi in cui $p\neq 1,2,\infty$.

Un'altra norma importante è quella di Frobenius (usata ad esempio nella definizione dell'errore per la regressione lineare):
\begin{equation}
||\mathbf{A}||_F = \sqrt{\sum_{i=1}^n \sum_{j=1}^m |A_{ij}^2|} = 
\sqrt{\text{tr}(\mathbf{A}\mathbf{A}^T)}
\end{equation}



\section{Gauss elimination}

L'eliminazione di Gauss consiste nell'eseguire operazioni sulla matrice che non cambiano il risultato del sistema di equazioni associato, come ad esempio \textit{scambiare delle righe, moltiplicare righe per scalari e sommare una riga all'altra}. Queste operazioni sono eseguite per semplificare il problema.

L'eliminazione di Gauss viene usata per portare la matrice ad una forma più facile da gestire, chiamata \textit{Row Echelon Form} (REF). In questa forma la matrice ha le seguenti proprietà:
\begin{itemize}[noitemsep]
\item il primo\footnote{Primo elemento non nullo della riga partendo da sinistra verso destra.} elemento non nullo di una riga (\emph{pivot}) è esattamente 1.0
\item righe che hanno tutti elementi nulli sono sotto le righe che hanno almeno un elemento non nullo
\item ogni pivot di una riga si trova ad una colonna alla destra dei pivot delle righe superiori
\end{itemize}
L'algoritmo che trasforma la matrice nella \textit{Row Echelon Form} è il seguente:
\begin{enumerate}[noitemsep]
\item Trova il pivot (primo elemento non nullo della riga). Se la colonna ha elementi tutti nulli passa alla colonna successiva.
\item Scambia le righe posizionando la riga del pivot come prima.
\item Moltiplica ogni elemento sulla riga per $\frac{1}{\text{pivot}}$ in maniera tale che il pivot è pari a 1.0.
\item Tramite la somma tra righe (premoltiplicando per il pivot dell'altra riga), fa in maniera tale che tutti gli elementi sulla colonna del pivot (elementi che non sono il pivot) sono pari a zero.
\item Continua il processo finché non si è giunti alla \textit{Row Echelon Form}.
\end{enumerate}

La forma \textit{Reduced Row Echelon Form} (RREF) consiste nell'avere solamente un valore diverso da zero per ogni colonna.
Si nota che una matrice può avere molte REF, ma solamente una RREF.


\subsection{Basic operations}

Le operazioni base dell'eliminazione di Gauss sono:
\begin{itemize}[noitemsep]
\item Scambio di due righe (row swapping: \verb|matd_row_swap_r|).
\item Moltiplica ogni elemento della riga per uno scalare diverso da zero (scalar multiplication of rows: \verb|matd_row_smul_r|).
\item Moltiplica una riga per in termine diverso da zero e aggiunge il risultato a un'altra riga (row addition: \verb|matd_row_addrow_r|).
\end{itemize}

\subsubsection{Row swapping}

\begin{lstlisting}
int matd_row_swap_r(matd *m, unsigned int row1, unsigned int row2)
{
	// swap two rows of matrix m
	int i;
	double tmp;
	if(row1 >= m->n_rows || row2 >= m->n_rows) return 0; // cannot swap rows
	for(i=0; i<m->n_cols; i++){
		tmp = m->data[row2*m->n_cols+i];
		m->data[row2*m->n_cols+i] = m->data[row1*m->n_cols+i];
		m->data[row1*m->n_cols+i] = tmp;
	}
	return 1;
}
\end{lstlisting}

Esempio:
\begin{equation*}
\mathtt{matd\_row\_swap\_r} = \left( 
\begin{bmatrix}
1 & 2 & 3 \\ 0 & 2 & 4 \\ 2 & 1 & 9
\end{bmatrix}, \; 0, \; 1
\right) = 
\begin{bmatrix}
0 & 2 & 4 \\ 1 & 2 & 3 \\ 2 & 1 & 9
\end{bmatrix}
\end{equation*}

\subsubsection{Scalar multiplication of rows}

\begin{lstlisting}
int matd_row_smul_r(matd *m, unsigned int row, double num)
{
	int i;
	if(row >= m->n_rows) return 0; // cannot row multiply
	for(i=0; i<m->n_cols; i++) m->data[row*m->n_cols+i] *= num;
	return 1;
}
\end{lstlisting}

Esempio:
\begin{equation*}
\mathtt{matd\_row\_swap\_r} = \left( 
\begin{bmatrix}
1 & 2 & 3 \\ 0 & 2 & 4 \\ 2 & 1 & 9
\end{bmatrix}, \; 1, \; 2.0
\right) = 
\begin{bmatrix}
1 & 2 & 3 \\ 0.0 & 4.0 & 6.0 \\ 2 & 1 & 9
\end{bmatrix}
\end{equation*}

\subsubsection{Row addition}

\begin{lstlisting}
int matd_row_addrow_r(matd *m, unsigned int where, unsigned int row, double multiplier)
{
	int i = 0;
	if(where >= m->n_rows || row >= m->n_rows) return 0; // cannot add rows
	for(i=0; i<m->n_cols; i++) m->data[where*m->n_cols+i] += multiplier * m->data[row*m->n_cols+i];
	return 1;
}
\end{lstlisting}

Esempio:
\begin{equation*}
\mathtt{matd\_row\_addrow\_r} = \left( 
\begin{bmatrix}
1 & 2 & 3 \\ 0 & 2 & 4 \\ 2 & 1 & 9
\end{bmatrix}, \; 0, \; 1, \; 0.5
\right) = 
\begin{bmatrix}
1+0\cdot 0.5 & 2+2\cdot 0.5 & 3+4\cdot 0.5 \\
0 & 2 & 4 \\
4 & 1 & 9
\end{bmatrix} = 
\begin{bmatrix}
1 & 3 & 5 \\ 0 & 2 & 4 \\ 2 & 1 & 9
\end{bmatrix}
\end{equation*}



\section{Matrix factorizations}

La fattorizzazione, o la decomposizione, di una matrice consiste nel \emph{dividere la matrice nel prodotto di matrici più semplici}.
In letteratura sono presenti varie decomposizioni applicabili a diverse classi di problemi:
\begin{description}
\item[Decomposizione LU:] La matrice $\mathbf{A}=\mathbf{L}\mathbf{U}$ è decomposta in una matrice triangolare inferiore $\mathbf{L}$ e una triangolare superiore $\mathbf{U}$. Applicabile a matrici quadrate.
\item[Decomposizione QR:] La matrice $\mathbf{A}=\mathbf{Q}\mathbf{R}$ é divisa in una matrice $\mathbf{Q}$ ortogonale e una $\mathbf{R}$ triangolare superiore di dimensione $n\times m$. Applicabile a matrici di dimensione qualsiasi $n\times m$.
\item[Decomposizione di Cholesky:] La matrice $\mathbf{A}=\mathbf{U}^T\mathbf{U}$ é scomposta tramite una matrice $\mathbf{U}$ triangolare superiore con elementi sulla diagonale positivi. Si applica a matrici quadrate, simmetriche, definite positive.
\item[Decomposizione a valori singolari:] La matrice $\mathbf{A}=\mathbf{U}\mathbf{D}\mathbf{V}^H$ é scomposta in una matrice $\mathbf{D}$ diagonale non-negativa e due matrici unitarie $\mathbf{U}$ e $\mathbf{V}$ ($\mathbf{V}^H$ denota la trasposta coniugata di $\mathbf{V}$). Applicabile a matrici di dimensione $n\times m$.
\item[Decomposizione spettrale:] La matrice $\mathbf{A}=\mathbf{V}\mathbf{D}\mathbf{V}^{-1}$ viene decomposta tramite la matrice $\mathbf{V}$ le cui colonne sono gli autovettori di $\mathbf{A}$ e la matrice $\mathbf{D}$ é diagonale con i corrispondenti autovalori come diagonale. Applicabile a matrici quadrate con autovettori distinti\footnote{non necessariamente anche distinti autovalori.}.
\item[Decomposizione di Schur:]
\item[Decomposizione in componenti principali:]
\item[Decomposizione non-negativa:] La matrice $\mathbf{A}=\mathbf{W}\mathbf{H}$ è decomposta in due matrici che possiedono elementi positivi. É una soluzione, in genere di minimo locale, della funzione obiettivo $f(\mathbf{W},\mathbf{H})=\frac{1}{2}||\mathbf{A}-\mathbf{W}\mathbf{H}||^2_F$ con i vincoli $W_{ij} \geq 0$ e $H_{hk}\geq 0$. Applicabile a matrici $n\times m$ non-negative.
\end{description}

\subsection{LU(P) decomposition}


La decomposizione LU, chiamata anche la fattorizzazione LU, si riferisce alla scomposizione di una matrice $\mathbf{A}$ in due matrici $\mathbf{L}$ e $\mathbf{U}$:
\begin{equation}
\mathbf{A} = \mathbf{L} \; \mathbf{U}
\qquad \rightarrow \qquad
\begin{bmatrix}
A_{11} & A_{12} & A_{13} \\
A_{21} & A_{22} & A_{23} \\
A_{31} & A_{32} & A_{33}
\end{bmatrix} = 
\begin{bmatrix}
1 & 0 & 0 \\
L_{21} & 1 & 0 \\
L_{31} & L_{32} & 1
\end{bmatrix}
\begin{bmatrix}
U_{11} & U_{12} & U_{13} \\
0      & U_{22} & U_{23} \\
0      & 0      & U_{33}
\end{bmatrix}
\end{equation}

In pratica questa fattorizzazione viene ricavata da operazioni elementari sulle righe (come lo scambio delle righe). In tal caso bisogna introdurre nell'equazione una matrice di permutazione $\mathbf{P}$ che tiene conto del cambio delle righe:
\begin{equation}
\mathbf{P} \; \mathbf{A} = \mathbf{L} \; \mathbf{U}
\label{eq:LUp}
\end{equation}
dove $\mathbf{P}$ è una matrice ricavata dalla permutazione delle righe di una matrice identità $\mathbf{I}$, ed è calcolata dalla procedura che trova $\mathbf{L}$ e $\mathbf{U}$.
Se una matrice $\mathbf{A}$ é quadrata (di dimensione $n\times n$) può sempre essere scomposta come $\mathbf{P} \; \mathbf{A} = \mathbf{L} \; \mathbf{U}$.
Per trovare la decomposizione LU si usa versione modificata dell'algoritmo per l'eliminazione di Gauss. Questa implementazione richiede circa $\frac{2}{3}n^3$ operazioni a virgola mobile. Altri algoritmi richiedono funzioni ricorsive o randomizzazione.

Trovare la decomposizione \eqref{eq:LUp} permette di calcolare il determinante della matrice $\mathbf{A}$, la sua inversa e, quindi, anche risolvere i sistemi di equazioni lineari.

Esiste anche un'altra fattorizzazione dove non solo le righe, ma anche le colonne vengono considerate e si chiama \emph{LU Factorization with full pivoting}.

\textcolor{red}{Descrizione dell'algoritmo e codice per la soluzione tramite decomposizione LU}

\textcolor{red}{Aggiungere quello che dicono su Wikipedia}

\subsection{QR decomposition}

Qualsiasi matrice simmetrica $\mathbf{A}$ può essere decomposta come:
\begin{equation}
\mathbf{A} = \mathbf{Q} \; \mathbf{R}
\label{eq:QR}
\end{equation}
dove $\mathbf{Q}$ è una matrice ortogonale\footnote{Una matrice si dice \emph{ortogonale} se
\begin{equation}
\mathbf{Q}\;\mathbf{Q}^T = \mathbf{Q}^T\mathbf{Q} = \mathbf{I}
\end{equation}
Questo equivale a dire che la trasposta della matrice è uguale all'inversa ($\mathbf{Q}^T=\mathbf{Q}^{-1}$).} e $\mathbf{R}$ una matrice triangolare superiore.
Per trovare la decomposizione \eqref{eq:QR} si usa l'algoritmo di Gram-Schmidt \textcolor{red}{sicuro?}.

Si può dimostrare che tutte le matrici quadrate ammettono una decomposizione QR, anche se essa non è unica. Nel caso in cui la matrice $\mathbf{A}$ sia a coefficienti complessi, allora $\mathbf{Q}$ è una matrice unitaria.

La fattorizzazione QR di una matrice richiede un tempo proporzionale a $O(n^3)$ (tramite trasformazioni di Householder o quelle di Givens). Nonostante la complessità computazionale è simile a quella della decomposizione LU (o algoritmo di Gauss) questo risulta avere una migliore stabilità numerica.
Inoltre la fattorizzazione QR può essere utilizzata per il calcolo di basi ortonormali e per la risoluzione di un sistema ai minimi quadrati. La decomposizione QR viene usata anche nel \textit{metodo QR} utilizzato per calcolare gli autovalori e autovettori di una matrice.

Al contrario della fattorizzazione LU(P) che si concentra sulle operazioni sulle righe, la decomposizione QR utilizza operazioni sulle colonne.
Considerando le matrici (in questo caso di dimensione $3\times 3$):
\begin{equation}
\mathbf{A} = 
\begin{bmatrix}
| & | & | \\
\mathbf{a}_1 & \mathbf{a}_2 & \mathbf{a}_3 \\
| & | & |
\end{bmatrix}
\qquad \qquad
\mathbf{Q} = 
\begin{bmatrix}
| & | & | \\
\mathbf{q}_1 & \mathbf{q}_2 & \mathbf{q}_3 \\
| & | & |
\end{bmatrix}
\end{equation}
dove $\mathbf{a}_i$ sono i vettori colonna che formano la matrice $\mathbf{A}$ e $\mathbf{q}_i$ i vettori colonna di $\mathbf{B}$.
La struttura della matrice $\mathbf{R}$ deriva dal fatto che $\mathbf{Q}$, essendo una matrice ortogonale, ha per colonne dei vettori unitari, ovvero di norma euclidea unitaria.
Questo porta alle seguenti formule:
\begin{equation}
\begin{dcases}
\mathbf{q}_1 = \frac{\mathbf{a}_1}{||\mathbf{a}_1||} \\
\mathbf{q}_2 = \frac{\mathbf{a}_2^\perp}{||\mathbf{a}_2^\perp||} \\
\mathbf{q}_2 = \frac{\mathbf{a}_3^\perp}{||\mathbf{a}_3^\perp||}
\end{dcases}
\qquad\qquad \text{dove} \qquad
\begin{array}{l}
\mathbf{a}_2^\perp  = \mathbf{a}_2 - \left<\mathbf{a}_2,\mathbf{q}_1\right> \mathbf{q}_1 \\ \\
\mathbf{a}_3^\perp  = \mathbf{a}_3 - \left<\mathbf{a}_3,\mathbf{q}_1\right> \mathbf{q}_1 - \left<\mathbf{a}_3,\mathbf{q}_2\right> \mathbf{q}_2
\end{array}
\end{equation}
dove $\left<\mathbf{u},\mathbf{v}\right>$ è il prodotto scalare tra i due vettori $\mathbf{u}$ e $\mathbf{v}$, mentre $||\mathbf{u}||$ è la norma-2 euclidea.
Quindi la decomposizione QR di questo esempio per matrici $3\times 3$ è pari a:
\begin{equation}
\mathbf{A} = 
\begin{bmatrix}
| & | & | \\
\mathbf{a}_1 & \mathbf{a}_2 & \mathbf{a}_3 \\
| & | & |
\end{bmatrix} = 
\begin{bmatrix}
| & | & | \\
\frac{\mathbf{a}_1}{||\mathbf{a}_1||} & \frac{\mathbf{a}_2^\perp}{||\mathbf{a}_2^\perp||} & \frac{\mathbf{a}_3^\perp}{||\mathbf{a}_3^\perp||} \\
| & | & |
\end{bmatrix}
\begin{bmatrix}
||\mathbf{a}_1|| & \left<\mathbf{a}_2,\mathbf{q}_1\right> & \left<\mathbf{a}_3,\mathbf{q}_1\right> \\
0 & ||\mathbf{a}_2^\perp|| & \left<\mathbf{a}_3,\mathbf{q}_2\right> \\
0 & 0 & ||\mathbf{a}_3^\perp||
\end{bmatrix}
\end{equation}
Questa formula può essere generalizzata al caso $n\times n$.


\subsection{Cholesky decomposition}

\subsection{Singular value decomposition (SVD)}

\subsection{Spectral decomposition}

\subsection{Schur decomposition}

\subsection{Principal component analysis (PCA)}

\subsection{Non-negative factorization}



\section{Solve linear systems}

Risolvere il sistema di equazioni lineari:
\begin{equation}
\mathbf{A} \; \mathbf{x} = \mathbf{b}
\label{eq:lin_sys}
\end{equation}
vuol dire trovare il vettore colonna incognito $\mathbf{x}$ sapendo la matrice $\mathbf{A}$ e il vettore dei termini noti $\mathbf{b}$. La soluzione è trovata in maniera esatta tramite la matrice inversa di $\mathbf{A}$:
\begin{equation}
\mathbf{x} = \mathbf{A}^{-1} \; \mathbf{b}
\end{equation}
Dal punto di vista dell'implementazione il principale problema risiede nel fatto che calcolare la matrice inversa$\mathbf{A}^{-1}$ attraverso il metodo di \textcolor{red}{Come si chiamava? (quello lentissimo esatto)} è un'operazione molto dispendiosa.
Per questo esistono molti metodi per calcolare velocemente il vettore ignoto $\mathbf{x}$ in maniera computazionalmente efficiente.



\subsection{Gauss-Jordan elimination}


\subsubsection{Backward substitution}

La sostituzione all'indietro è la tecnica per la quale si calcola la soluzione del sistema lineare $\mathbf{U}\mathbf{x}=\mathbf{b}$ dove $\mathbf{U}$ è una matrice triangolare superiore:
\begin{equation}
\mathbf{U} \; \mathbf{x} = \mathbf{b}
\qquad \rightarrow \qquad
\begin{bmatrix}
U_{11} & U_{12} & \cdots & U_{1n} \\
0      & U_{22} & \cdots & U_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
0      & 0      & \cdots & U_{nn}
\end{bmatrix}
\begin{bmatrix}
x_1 \\ x_2 \\ \vdots \\ x_n
\end{bmatrix} = 
\begin{bmatrix}
b_1 \\ b_2 \\ \vdots \\ b_n
\end{bmatrix}
\end{equation}
Si noti che l'ultimo termine del vettore $\mathbf{x}$ è facilmente calcolabile dall'ultima equazione (quella corrispondente alla $n$-esima riga). Questa equazione permette di trovare $x_n$ e poi si procede \emph{all'indietro} per trovare le altre componenti del vettore delle incognite $\mathbf{x}$. Per questo motivo è chiamato \emph{metodo della sostituzione all'indietro}.
\begin{equation}
x_n = \frac{b_n}{U_{nn}} \qquad
x_{n-1} = \frac{b_{n-1} - U_{(n-1)n} x_n}{U_{(n-1)(n-1)}} \qquad
x_i = \frac{b_i - \sum_{k=i}^{n-1} U_{ik} x_k}{U_{ii}}
\end{equation}

\begin{lstlisting}
matd *lu_solvebck(matd *U, matd *b)
{
	matd *x = new_matd(U->n_cols, 1);
	int i = U->n_cols, j;
	double tmp;
	while(i-- > 0){
		tmp = b->data[i*b->n_cols];
		for(j=i; j<U->n_cols; j++) tmp -= U->data[i*U->n_cols+j] * x->data[j*x->n_cols];
		x->data[i*x->n_cols] = tmp / U->data[i*U->n_cols+i];
	}
	return x;
}
\end{lstlisting}


\subsubsection{Forward substitution}

Questo è il caso opposto alla sostituzione all'indietro, ovvero un metodo per trovare la soluzione del sistema lineare $\mathbf{L}\mathbf{x}=\mathbf{b}$ dove $\mathbf{L}$ è, al contrario del caso precedente, una matrice triangolare inferiore.
\begin{equation}
\mathbf{L} \; \mathbf{x} = \mathbf{b}
\qquad \rightarrow \qquad
\begin{bmatrix}
L_{11} & 0      & \cdots & 0      \\
L_{21} & L_{22} & \cdots & 0      \\
\vdots & \vdots & \ddots & \vdots \\
L_{n1} & L_{n2} & \cdots & L_{nn}
\end{bmatrix}
\begin{bmatrix}
x_1 \\ x_2 \\ \vdots \\ x_n
\end{bmatrix} = 
\begin{bmatrix}
b_1 \\ b_2 \\ \vdots \\ b_n
\end{bmatrix}
\end{equation}
La sostituzione in avanti risulta essere, quindi, pari a:
\begin{equation}
x_1 = \frac{b_1}{L_{11}} \qquad
x_2 = \frac{b_2 - L_{21} x_1}{L_{22}} \qquad
x_i = \frac{b_i - \sum_{k=1}^{i-1} L_{ik} x_k}{L_{ii}}
\end{equation}

\begin{lstlisting}
matd *lu_solvefwd(matd *L, matd *b)
{
	matd *x = new_matd(L->n_cols, 1);
	int i,j;
	double tmp;
	for(i=0; i<L->n_cols; i++){
		tmp = b->data[i*b->n_cols];
		for(j=0; j<i; j++){
			tmp -= L->data[i*L->n_cols+j] * x->data[j*x->n_cols];
		}
		x->data[i*x->n_cols] = tmp / L->data[i*L->n_cols+i];
	}
	return x;
}
\end{lstlisting}


\subsection{LU(P) decomposition}

Usando la decomposizione \eqref{eq:LUp} il sistema di equazioni \eqref{eq:lin_sys} diventa:
\begin{equation}
\mathbf{A} \; \mathbf{x} = \mathbf{b}
\qquad \rightarrow \qquad
\mathbf{P} \; \mathbf{A} \; \mathbf{x} = \mathbf{P} \; \mathbf{b}
\qquad \rightarrow \qquad
\mathbf{L}\ \; \mathbf{U} \; \mathbf{x} = \mathbf{P} \; \mathbf{b}
\end{equation}
Introducendo il sistema di equazioni ausiliario $\mathbf{y} = \mathbf{U} \; \mathbf{x}$ troviamo la soluzione $\mathbf{x}$ del sistema lineare \eqref{eq:lin_sys} risolvendo i due sistemi:
\begin{align}
\mathbf{y} & = \mathbf{U} \; \mathbf{x} \\
\mathbf{L} \; \mathbf{y} & = \mathbf{P} \; \mathbf{b}
\end{align}
Per risolvere questi due sistemi di equazioni lineari si procede a trovare il vettore temporaneo $\mathbf{y}$ tramite sostituzione in avanti (\textit{forward substitution}) del sistema $\mathbf{L} \; \mathbf{y} = \mathbf{P} \; \mathbf{b}$, dato che $\mathbf{L}$ è una matrice triangolare inferiore. Successivamente si trova la soluzione $\mathbf{x}$ tramite il sistema $\mathbf{U} \; \mathbf{x} = \mathbf{y}$ tramite sostituzione all'indietro\footnote{All'indietro, nel senso che parte trovando l'ultimo termine di $\mathbf{x}$ e va all'indietro fino al primo $x_1$. Il contrario rispetto alla \textit{forward substitution}.} (\textit{backward substitution}), dato che la matrice $\mathbf{U}$ è triangolare superiore.

\begin{lstlisting}
matd *lu_solve(matd_lup *lu, matd* b)
{
	matd *Pb, *x, *y;
	if(lu->U->n_rows != b->n_rows || b->n_cols != 1) return NULL;
	Pb = matd_mul(lu->P, b);
	
	y = lu_solvefwd(lu->L, Pb); // Solve L*y = P*b using forward substition
	x = lu_solvebck(lu->U, y); // Solve U*x=y using backward substitution
	
	free_mat(y); // free memory space of the temporary matrices	
	free_mat(Pb);
	return x;
}
\end{lstlisting}




\subsection{QR decomposition}

Fattorizzata la matrice $\mathbf{A}=\mathbf{Q}\mathbf{R}$ di un sistema lineare $\mathbf{A}\mathbf{x}=\mathbf{b}$ la soluzione di tale sistema è data da:
\begin{equation}
\mathbf{x} = \mathbf{R}^{-1} (\mathbf{Q}^T \mathbf{b})
\end{equation}
Il calcolo del vettore temporaneo $\mathbf{y} = \mathbf{Q}^T \mathbf{b}$ richiede $O(n^2)$ operazioni per la moltiplicazione matrice-vettore, mentre il calcolo di $\mathbf{x}=\mathbf{R}^{-1}\mathbf{y}$ si può eseguire attraverso un algoritmo di sostituzione all'indietro con un tempo computazionale sempre di $O(n^2)$ operazioni. Il costo computazionale maggiore, dunque, è quello della fattorizzazione (pari a $O(n^3)$ operazioni).





\subsection{Conjugate gradient method}

Il metodo del gradiente coniugato in matematica serve per trovare iterativamente la soluzione di sistemi lineari definiti da una matrice positiva.
Spesso è usato per matrici definite positive sparse che hanno una dimensione eccessiva per poterli risolvere convenientemente con metodi diretti come la decomposizione di Cholesky.
Il metodo del gradiente coniugato è usato anche per risolvere problemi di ottimizzazione non-condizionati.

Il metodo del gradiente bi-coniugato provvede la generalizzazione a matrici non-simmetriche.
Esistono anche metodi del gradiente coniugato non-lineari.

Supponiamo di avere un sistema lineare~\eqref{eq:lin_sys} con vettore $\mathbf{x}$ di incognite di dimensione $n$ e con la matrice $\mathbf{A} \in \mathbb{K}^{n\times n}$ simmetrica, ovvero con $\mathbf{A}^T=\mathbf{A}$, definita positiva, ovvero con $\mathbf{x}^T\mathbf{A}\mathbf{x}>0$ per ogni vettore non-nullo $\mathbf{x}\in\mathbb{K}^n$ e reale.

Questo metodo può essere visto come una variazione del metodo iterativo di Arnoldi/Lanczos per il problema agli autovalori.

Diciamo che due vettori $\mathbf{u}$ e $\mathbf{v}$ sono coniugati rispetto a $\mathbf{A}$ se:
\begin{equation}
\mathbf{u}^T \mathbf{A} \mathbf{v} = 0
\end{equation}
Dal momento che $\mathbf{A}$ è simmetrica e definita positiva, la parte a sinistra dell'uguale definisce un prodotto interno:
\begin{equation}
\mathbf{u}^T\mathbf{A}\mathbf{v} = \left<\mathbf{u},\mathbf{v}\right>_\mathbf{A} :=
\left<\mathbf{Au},\mathbf{v}\right> = \left<\mathbf{u},\mathbf{A}^T\mathbf{v}\right> = \left<\mathbf{u}, \mathbf{Av}\right>
\end{equation}
Due vettori sono coniugati rispetto ad $\mathbf{A}$ se e solo se sono ortogonali rispetto a questo prodotto interno (se il prodotto interno è pari a zero).

Supponiamo che $P=\{\mathbf{p}_1, \mathbf{p}_2, \ldots, \mathbf{p}_n\}$ è un insieme di $n$ vettori mutuamente coniugati rispetto a $\mathbf{A}$, ovvero che $\mathbf{p}_i^T\mathbf{A}\mathbf{p}_j=0$ per ogni $i\neq j$. Allora $P$ forma una base di $\mathbb{K}^n$ ed esprimiamo le soluzioni $\mathbf{x}^*$ del sistema $\mathbf{Ax}=\mathbf{b}$ in questa base:
\begin{equation}
\mathbf{x}^* = \sum_{i=1}^n \alpha_i \mathbf{p}_i
\qquad \rightarrow \qquad
\mathbf{A}\mathbf{x}^* = \sum_{i=1}^n \alpha_i \mathbf{A}\mathbf{p}_i
\end{equation}
Moltiplicando a sinistra il sistema $\mathbf{Ax}=\mathbf{b}$ con il vettore $\mathbf{p}_k^T$ si ottiene:
\begin{equation}
\mathbf{p}_k^T \mathbf{b} = \mathbf{p}_k^T \mathbf{A} \mathbf{x}^* = \sum_{i=1}^n \alpha_i \mathbf{p}_k^T \mathbf{A} \mathbf{p}_i = \sum_{i=1}^n \alpha_i \left<\mathbf{p}_k,\mathbf{p}_i\right>_\mathbf{A} = \alpha_k \left<\mathbf{p}_k,\mathbf{p}_k\right>_\mathbf{A}
\end{equation}
e quindi otteniamo:
\begin{equation}
\alpha_k = \frac{\left<\mathbf{p}_k,\mathbf{b}\right>}{\left<\mathbf{p}_k,\mathbf{p}_k\right>}
\end{equation}
Quindi il metodo per risolvere il sistema \eqref{eq:lin_sys} consiste nel trovare una sequenza $P$ di $n$ direzioni coniugate e poi calcolare i coefficienti $\alpha_k$.

Se scegliamo i vettori $\mathbf{p}_k$ adeguatamente, allora non necessitiamo neanche di tutti i vettori per avere una approssimazione soddisfacente della soluzione $\mathbf{x}$.

Definiamo il valore iniziale di $\mathbf{x}^*$ come $\mathbf{x}_0$ (inizialmente possiamo porre $\mathbf{x}_0=\mathbf{0}$).
La funzione obiettivo da minimizzare è la seguente forma quadratica:
\begin{equation}
f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^T \mathbf{A} \mathbf{x} - \mathbf{x}^T \mathbf{b}
\end{equation}
Il gradiente (derivata prima) è pari a:
\begin{equation}
\nabla f(\mathbf{x}) = \mathbf{Ax}-\mathbf{b}
\end{equation}
mentre la derivata seconda è pari alla matrice Hessiana $\mathbf{H}(f(\mathbf{x})) = \mathbf{A}$ che per definizione della matrice $\mathbf{A}$ è anch'essa definita positiva.

Questo suggerisce a prendere come vettore $\mathbf{p}_0$ il negativo del gradiente di $f$ a $\mathbf{x}=\mathbf{x}_0$. Questo vuol dire che $\mathbf{p}_0=\mathbf{b}-\mathbf{Ax}_0$. Gli altri vettori saranno coniugati al gradiente (da qui il nome del metodo). Si noti inoltre che $\mathbf{p}_0$ è anche il residuo calcolato dal primo step dell'algoritmo.
Sia $\mathbf{r}_k$ il residuo al $k$-esimo step (anche uguale al negativo del gradiente di $f$ al $k$-esimo step):
\begin{equation}
\mathbf{r}_k = \mathbf{b} - \mathbf{Ax}_k
\end{equation}
lo step successivo dovrà muoversi in direzione $\mathbf{r}_k$ dove le altre direzioni sono coniugate l'una all'altra.
Una maniera pratica per assicurarsi questo è trovare le direzioni successive dal residuo corrente e da tutte le direzioni precedenti. Questo metodo di calcolare il vincolo può essere visto come un esempio dell'ortonormalizzazione di Gram-Schmidt:
\begin{equation}
\mathbf{p}_k = \mathbf{r}_k - \sum_{i<k} \frac{\mathbf{p}_i^T\mathbf{A}\mathbf{r}_k}{\mathbf{p}_i^T\mathbf{A}\mathbf{p}_i}\mathbf{p}_i
\end{equation}
Seguendo questa direzione la seguente posizione ottimale è:
\begin{equation}
\mathbf{x}_{k+1} = \mathbf{x}_k + \alpha_k \mathbf{p}_k
\end{equation}
con
\begin{equation}
\alpha_k = \frac{\mathbf{p}_k^T(\mathbf{b}-\mathbf{Ax}_k)}{\mathbf{p}_k^T\mathbf{Ap}_k} = 
\frac{\mathbf{p}_k^T\mathbf{r}_k}{\mathbf{p}_k^T\mathbf{Ap}_k}
\end{equation}
che deriva dall'aver posto pari a zero la derivata di $f(\mathbf{x}_{k+1})$ rispetto a $\alpha_k$.


\begin{algorithm}
\caption{Conjugate gradient}\label{alg:mul_simple}
\begin{algorithmic}[1]
\Require matrix $\mathbf{A}$, vector $\mathbf{b}$
\State $\mathbf{r}_0 \gets \mathbf{b} - \mathbf{Ax}_0$
\State $\mathbf{p}_0 \gets \mathbf{r}_0$
\State $k \gets 0$
\While{$\mathbf{r}_k$ is sufficiently small}
	\State $\alpha_k \gets \frac{\mathbf{p}_k^T\mathbf{r}_k}{\mathbf{p}_k^T\mathbf{Ap}_k}$
	\State $\mathbf{x}_{k+1} \gets \mathbf{x}_k + \alpha_k \mathbf{p}_k$
	\State $\mathbf{r}_{k+1} \gets \mathbf{r}_k - \alpha_k \mathbf{Ap}_k$
	\State $\beta_k \gets \frac{\mathbf{r}_{k+1}^T\mathbf{r}_{k+1}}{\mathbf{r}_k^T\mathbf{r}_k}$
	\State $\mathbf{p}_{k+1} \gets \mathbf{r}_{k+1} + \beta_k \mathbf{p}_k$
	\State $k \gets k + 1$
\EndWhile
\State return $\mathbf{x}_{k+1}$ as the result
\end{algorithmic}
\end{algorithm}

Nell'algoritmo $\alpha_k$ è scelto in maniera tale che $\mathbf{t}_{k+1}$ è ortogonale a $\mathbf{r}_k$

Codice Matlab
\begin{verbatim}
function x = conjgrad(A, b, x)
    r = b - A * x;
    p = r;
    rsold = r' * r;

    for i = 1:length(b)
        Ap = A * p;
        alpha = rsold / (p' * Ap);
        x = x + alpha * p;
        r = r - alpha * Ap;
        rsnew = r' * r;
        if sqrt(rsnew) < 1e-10
            break
        end
        p = r + (rsnew / rsold) * p;
        rsold = rsnew;
    end
end
\end{verbatim}



\section{Determinante}

\subsection{Formula di Leibniz}

il determinante di una matrice $\mathbf{A}$ di dimensione $n\times n$ è trovato tramite la seguente formula:
\begin{equation}
\det(\mathbf{A}) = \sum_{\sigma\in S_n} \text{sgn}(\sigma) \prod_{i=1}^n A_{i\;\sigma(i)}
\end{equation}
dove $S_n$ è l'insieme di tutte le permutazioni $\sigma$ dell'insieme ${1,2,3,\ldots,n}$, mentre $\text{sgn}(\sigma)$ denota il segno della permutazione (+1 se è una permutazione pari, -1 se è dispari).
Questo metodo ha un elevato costo computazionale, pari a $O(N\cdot N!)$.

\subsection{Sviluppo di Laplace}

Questo metodo risulta efficace solamente se la matrice contiene un gran numero di zeri. Scegliendo una riga $i$ si calcola il determinante come:
\begin{equation}
\det(\mathbf{A}) = \sum_{j=1}^n A_{ij} \; C_{ij}
\end{equation}
dove $C_{ij}$ è il complemento algebrico, ovvero è pari a $(-1)^{i+j}$ per il determinante del minore di ordine $n-1$ ottenuto dalla matrice $\mathbf{A}$ eliminando la riga $i$-esima e la colonna $j$-esima.
Il metodo risulta efficiente in caso si scelga una riga $i$ che contiene il maggior numero di zeri possibile.

Lo stesso sviluppo può essere eseguito su una determinata colonna.

\subsection{Algoritmo di Gauss}

Nel caso di matrici triangolari (siano esse triangolari superiori o inferiori) il determinante è pari al prodotto degli elementi sulla diagonale principale. Questo deriva anche dallo sviluppo di Laplace.




\section{Inverse matrix}

L'algoritmo peggiore in termini computazionali per trovare la matrice inversa fa uso della matrice aggiunta
\begin{equation}
\mathbf{A}^{-1} = \frac{\text{adj}(\mathbf{A})}{\det(\mathbf{A})}
\end{equation}
La matrice $\text{adj}(\mathbf{A})$ è calcolata tramite la trasposta della matrice dei cofattori $\text{adj}(\mathbf{A}) = \text{cof}(\mathbf{A})^T$. La matrice dei cofattori $\text{cof}(\mathbf{A})$ ha elementi:
\begin{equation}
\text{cof}_{ij}(\mathbf{A}) = (-1)^{i+j} \det\left(\mathbf{M}_{ij}\right)
\end{equation}
dove $\mathbf{M}_{ij}$ è la matrice minore di $\mathbf{A}$, ottenuta eliminando la $i$-esima riga e la $j$-esima colonna della matrice $\mathbf{A}$.




\section{Eigen}

\begin{tabular}{lllcll}
\textbf{metodo} & \textbf{si applica a} & \textbf{produce} & \textbf{costo} & \textbf{conv.} & \textbf{descrizione} \\
\hline
Lanczos algorithm & Hermitian & $m$ largest/smallest eigenpair & & & \\
Power iteration & general & eigenpair with largest value & $O(n^2)$ & linear & applica la matrice ripetutamente ad un vettore iniziale e rinormalizza \\
Inverse iteration & general & eigenpair with value closest to $\mu$ &  & linear & power iteration applicata a $(\mathbf{A} - \mu \mathbf{I})^{-1}$ \\
Rayleigh quotient iteration & hermitian & any eigenpair & & cubica & power iteration applicata a $(\mathbf{A} - \mu \mathbf{I})^{-1}$, dove $\mu$ ad ogni iterazione è il quoziente di Rayleigh della precedente \\
Preconditioned inverse iteration & Hermitiana & any eigenpair & & cubic & inverse iteration usando un precondizionatore (una specie di inversa di $\mathbf{A}$). Chiamato anche LOBPCG algorithm \\
Bisection method & real symmetric tridiagonal & any eigenvalue & & lineare & usa il metodo della bisezione per trovare le soluzioni del polinomio caratteristico, supportato dalla sequenza di Sturm \\
Laguerre iteration & real symmetric tridiagonal & any eigenvalue & & cubic & metodo di Laguerre per trovare le soluzioni del polinomio caratteristico fornito dalla sequenza di Sturm \\
QR algorithm & Hassenberg & all eigenvalues and all eigenpairs & $O(n^2)$ e $6n^3+O(n^2)$ & cubic & applica l'iterazione successiva a $\mathbf{R}\mathbf{Q}$ \\

\end{tabular}

\subsection{QR algorithm}

L'algoritmo più affidabile e maggiormente utilizzato è quello inventato da John G.F. Francis alla fine degli anni '60 che sfrutta la decomposizione QR.

L'algoritmo parte dalla matrice $\mathbf{A}$ di cui vogliamo calcolare gli autovalori e iterativamente arriva a triangolarizzarla. Allo step iniziale $k=0$ la matrice $\mathbf{A}_0=\mathbf{A}$ è posta pari alla matrice di input. Al generico step $k$ si procede alla decomposizione QR $\mathbf{A}_k=\mathbf{Q}_k\mathbf{R}_k$. Quindi allo step successivo la matrice $\mathbf{A}_{k+1}$ è trovata tramite:
\begin{equation}
\mathbf{A}_{k+1} = \mathbf{R}_k\mathbf{Q}_k = \mathbf{Q}_k^{-1}\mathbf{Q}_k\mathbf{R}_k\mathbf{Q}_k = 
\mathbf{Q}_k^{-1}\mathbf{A}_k\mathbf{Q}_k = \mathbf{Q}_k^T\mathbf{A}_k\mathbf{Q}_k
\end{equation}
Notare che tutte le matrici $\mathbf{A}_k$ sono simili e quindi hanno sempre gli stessi autovalori. L'algoritmo è numericamente stabile perché procede tramite trasformazioni simili ortogonali.
Sotto certe condizioni le matrici $\mathbf{A}_k$ convergono ad una matrice triangolare, la \emph{forma di Schur} di $\mathbf{A}$ (gli autovalori sono gli elementi sulla diagonale).

In questa forma grezza il calcolo delle iterazioni può essere computazionalmente oneroso e questo può essere ridotto trasformando inizialmente la matrice $\mathbf{A}$ in una forma di Hessenberg superiore. Questa trasformazione costa $\frac{10}{9}n^3+O(n^2)$ operazioni aritmetiche usando una riduzione di Householder. Il calcolo della decomposizione QR di una matrice di Hessenberg superiore ha un costo di $6n^2+O(n)$. Inoltre essendo la forma di Hessenberg una sorta di triangolare superiore questo riduce anche il numero di iterazioni per arrivare a convergenza.





\newpage

\appendix

\section{Random numbers}

\textcolor{red}{Algoritmi di generazione dei numeri casuali}

\subsection{Sampling from Gaussian distribution}

\textcolor{red}{metodi di sampling descritti sul mio quaderno dei ponti integrali, con qualche indicazione in piu`}



\end{document}